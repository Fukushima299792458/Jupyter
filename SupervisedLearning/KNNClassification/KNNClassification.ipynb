{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# KNN Classification\n",
                "\n",
                "K-Nearest Neighbours (KNN) classification is a form of supervised learning used to find patterns in less structured labelled data. Similar to regression, indeed the KNN algorithm can be used for a different kind of non-linear regression, the KNN algorithm plots the data points in multidimensional vectors. However, unlike regression where a continuous prediction interface is calculated between continuous input fields and continuous output fields, in classification, the output is discrete/integer and often binary e.g. diabetes or no diabetes, spam or not spam etc. \n",
                "\n",
                "Each value within the vector space is assigned some discrete class, then, when a new value is added to classify, the KKN algorithm finds the k-nearest neighbours and finds the class with the most points neighbouring the new point. The new point is then predicted to be in this class. This plurality vote system is based on the assumption that points that are near each other in the vector space are more likely to have the same class. This can be useful for quickly classifying new data based on predetermined classes.\n",
                "\n",
                "However, because KNN classification needs to calculate the distance to all of the points in the vector space, this algorithm doesn't scale exceptionally well in its most basic form for large datasets. Additionally, the discrete nature of the algorithm does mean it can't distinguish the more spectral classifications and doesn't work as well for data that isn't already clearly defined and separated. Finally, by definition, as a supervised learning algorithm it cannot add any categories outside of what it has been taught unlike [K-means clustering](../UnsupervisedLearning).\n",
                "\n",
                "## Importing the Libraries\n",
                "\n",
                "First, the libraries need to be imported. If you don't have these libraries installed, run \n",
                "```\n",
                "pip install pandas\n",
                "pip install scikit-learn\n",
                "pip install math\n",
                "```\n",
                "in terminal. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 515,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd; # Used for dealing with dataset(s).\n",
                "import sklearn; # Performs the complex mathematics required for some parts of the code making the process of programming the AI much less laborious.\n",
                "import math; # Used for certain maths functions like the square root.\n",
                "from sklearn.neighbors import KNeighborsClassifier; # Get the algorithm for k-nearest neighbours classification."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Importing the Data\n",
                "\n",
                "After the libraries have been imported we extract the example dataset from the data downloaded from the [UCI database](https://archive.ics.uci.edu/dataset/19/car+evaluation). This dataset is separated by commas as is the norm, however, some datasets are separated by semicolons or other punctuation marks and this needs to be specified. Then we check that all the data got imported properly by printing the 'head' or the top few values from the dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 516,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1728\n",
                        "  buyingChance mAIntenance doors people   boot safety  class\n",
                        "0        vhigh       vhigh     2      2  small    low  unacc\n",
                        "1        vhigh       vhigh     2      2  small    med  unacc\n",
                        "2        vhigh       vhigh     2      2  small   high  unacc\n",
                        "3        vhigh       vhigh     2      2    med    low  unacc\n",
                        "4        vhigh       vhigh     2      2    med    med  unacc\n"
                    ]
                }
            ],
            "source": [
                "data = pd.read_csv(\"car.data\", sep=\",\"); # Import the labelled data from the .data file and convert it into the array, separating each data point based on the \",\".\n",
                "print(len(data)); # Print the length of the dataset. This may be useful to determine how large your dataset is, if your dataset is too small, or too large\n",
                "print(data.head()); # Print the first half a dozen values from the data to check that the data was imported properly."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Cleaning\n",
                "\n",
                "As you can see above, some of the data is in strings but the algorithm can only work with numbers so we need to loop through each of the columns and check if it contains any of the common words and replace them with numerical values. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 517,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "C:\\Users\\Phoen\\AppData\\Local\\Temp\\ipykernel_24824\\2562637764.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
                        "  data[column] = data[column].replace(\"vhigh\", 3);\n",
                        "C:\\Users\\Phoen\\AppData\\Local\\Temp\\ipykernel_24824\\2562637764.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
                        "  data[column] = data[column].replace(\"big\", 2);\n",
                        "C:\\Users\\Phoen\\AppData\\Local\\Temp\\ipykernel_24824\\2562637764.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
                        "  data[column] = data[column].replace(\"high\", 2);\n",
                        "C:\\Users\\Phoen\\AppData\\Local\\Temp\\ipykernel_24824\\2562637764.py:20: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
                        "  data[column] = data[column].replace(\"vgood\", 3);\n"
                    ]
                }
            ],
            "source": [
                "for column in data.columns: # Loop through each column of the data.\n",
                "    # Replace the level strings with integers.\n",
                "    data[column] = data[column].replace(\"low\", 0); \n",
                "    data[column] = data[column].replace(\"med\", 1); \n",
                "    data[column] = data[column].replace(\"high\", 2); \n",
                "    data[column] = data[column].replace(\"vhigh\", 3); \n",
                "    \n",
                "    data[column] = data[column].replace(\"more\", 5); # Replace more in the people column with 5 (one more than the highest value of 4).\n",
                "\n",
                "    data[column] = data[column].replace(\"5more\", 5); # Replace 5more in the doors column with 5 (one more than the highest value of 4).\n",
                "\n",
                "    # Replace the boot sizes with integers (med size is replaced in the first set already).\n",
                "    data[column] = data[column].replace(\"small\", 0); \n",
                "    data[column] = data[column].replace(\"big\", 2); \n",
                "\n",
                "    # Replace the acceptability/class with integers.\n",
                "    data[column] = data[column].replace(\"unacc\", 0); \n",
                "    data[column] = data[column].replace(\"acc\", 1); \n",
                "    data[column] = data[column].replace(\"good\", 2); \n",
                "    data[column] = data[column].replace(\"vgood\", 3); "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## TrAIng Data vs Testing Data\n",
                "\n",
                "In machine learning, to prevent overfitting, the data is often split into two partitions; testing data and training data.\n",
                "\n",
                "Overfitting is when the machine learning algorithm effectively 'memorises' the training data so that, while it achieves very high scores on the training data, it fails to effectively predict real-world data. As the point of AI is to predict patterns in the real world, this is obviously not very good. To control for this, usually about 20% of the data will be removed from the training data and used to test the accuracy of the AI on data it has never seen. The remaining data is then given to the AI for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 518,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split the dataset into train and test\n",
                "x = data.iloc[:, 0:6]; # Keeps all columns, excluding column 6. This is the data that will be used to predict.\n",
                "y = data.iloc[:, 6]; # Seperates column 6, the acceptability of the car. This is the value to be predicted. \n",
                "x_trAIn, x_test, y_trAIn, y_test = sklearn.model_selection.trAIn_test_split(x, y, random_state=0, test_size=0.3); # Seperate the data into the training data and testing data at a ratio of 4:1. Really the data is split into training data used to predict (x_train), the training data to be predicted (y_train), the testing data used to predict (x_test), and the testing data to be predicted (y_test). "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 519,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Attribute scaling\n",
                "sc_x = sklearn.preprocessing.StandardScaler(); # A scaler to confine/scale all data between -1 and 1 to keep the data within the bounds of the KNN classification algorithm.\n",
                "x_trAIn = sc_x.fit_transform(x_trAIn); # Scale the trAIning data between -1 and 1 using the scaler defined above.\n",
                "x_test = sc_x.transform(x_test); # Scale the testing data between -1 and 1 using the scaler defined above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 520,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "519\n",
                        "22.781571499789035\n"
                    ]
                }
            ],
            "source": [
                "print(len(y_test)); # Print length of test data.\n",
                "print(math.sqrt(len(y_test))); # Print the square root of the length of the test data as this often serves as a good indicator of approximately what the hyperparameter should be (at least within an order of magnitude)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Refining K\n",
                "\n",
                "As the k hyperparameter is so central to this classification method, it makes sense that using the correct k value is extremely important for the model accuracy and precision. A large k parameter will sample too wide an area and get an inpricise, and overly smoothed and underfitted model that doesn't take local patterns into account when making classifications. Alternatively, a small k value will lead to less accurate results that are more sensitive to random outliers and noisy data as the model will only take into account the closest neigbours making it more likely to misidentify a new data point because of slightly more random outliers in that exact spot. Finally, the model should have a k parameter that is odd, and preferably some values between the number of classes and 2 as otherwise there can be an equal number of two or more classes within the range of the k nearest neigbours which leads to ambiguity in the plurality voting system and lead to lower accuracy. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 521,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the model: Init K-NN\n",
                "classifier = KNeighborsClassifier(n_neighbors=7, p=2, metric='euclidean'); # Create the KNN classifying algorithm and set the k hyperparameter as outlined above.\n",
                "classifier.fit(x_trAIn, y_trAIn); # TrAIn the KNN classification algorithm based on the training data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 522,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[0 0 0 0 0 2 0 1 1 0 0 0 1 0 0 1 0 0 1 0 1 1 2 0 0 0 0 0 0 0 3 1 0 1 2 0 0\n",
                        " 0 0 0 0 0 0 0 3 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1\n",
                        " 1 1 0 0 1 0 1 1 1 0 0 0 0 2 0 0 0 1 0 0 0 3 0 1 1 0 0 0 1 0 3 1 0 1 0 0 0\n",
                        " 0 0 1 0 2 1 0 0 0 0 0 1 0 0 0 0 1 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
                        " 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1\n",
                        " 0 2 0 0 0 0 1 0 0 0 1 0 2 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1\n",
                        " 0 3 0 0 0 3 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 0 0 0 0 2 1 1 1 1 0 0\n",
                        " 0 1 0 0 1 0 0 3 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 1 0 3 2 0 0 0 0 0 0 0\n",
                        " 0 0 0 1 0 1 1 2 2 0 1 0 0 0 2 1 0 0 0 0 0 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 2\n",
                        " 0 2 0 0 0 3 0 0 3 0 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 2 3 1 1 0 1\n",
                        " 0 0 0 1 0 0 0 0 0 1 2 0 2 0 0 0 1 0 0 0 0 0 0 3 0 0 1 0 0 1 0 1 0 0 3 0 0\n",
                        " 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 3 0 0 0 3 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0\n",
                        " 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2 0 1 0 0 0 1 0 0 0 0 1 1 0 0\n",
                        " 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 0 2 0 0 1 0 2 0 0 2 0 0 0 0 1 0 0 0 0 0 0 1\n",
                        " 1]\n"
                    ]
                }
            ],
            "source": [
                "y_pred = classifier.predict(x_test); # Get the predicted outputs for the test data from the AI.\n",
                "print(y_pred); # Print the predicted outputs to make sure everything is working as intended."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Accuracy\n",
                "\n",
                "Above the accuracy as printed below is a confusion matrix that has numbers counting each of the cases as below.\n",
                "\n",
                "### AI Correctness Reference Table\n",
                "\n",
                "<table>\n",
                "    <tr>\n",
                "        <th>Actual</th>\n",
                "        <th>Predicted Unacceptable</th>\n",
                "        <th>Predicted Acceptable</th>\n",
                "        <th>Predicted Good</th>\n",
                "        <th>Predicted Very Good</th>\n",
                "    </tr>\n",
                "    <tr>\n",
                "        <th>Unacceptable</th>\n",
                "        <td><b>Correct</b></td>\n",
                "        <td>Incorrect</td>\n",
                "        <td>Incorrect</td>\n",
                "        <td>Incorrect</td>\n",
                "    </tr>\n",
                "    <tr>\n",
                "        <th>Acceptable</th>\n",
                "        <td>Incorrect</td>\n",
                "        <td><b>Correct</b></td>\n",
                "        <td>Incorrect</td>\n",
                "        <td>Incorrect</td>\n",
                "    </tr>\n",
                "    <tr>\n",
                "        <th>Good</th>\n",
                "        <td>Incorrect</td>\n",
                "        <td>Incorrect</td>\n",
                "        <td><b>Correct</b></td>\n",
                "        <td>Incorrect</td>\n",
                "    </tr>\n",
                "    <tr>\n",
                "        <th>Very Good</th>\n",
                "        <td>Incorrect</td>\n",
                "        <td>Incorrect</td>\n",
                "        <td>Incorrect</td>\n",
                "        <td><b>Correct</b></td>\n",
                "    </tr>\n",
                "</table>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 523,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[[361   2   0   0]\n",
                        " [  4 110   1   0]\n",
                        " [  0   2  22   1]\n",
                        " [  0   2   0  14]]\n",
                        "Accuracy: 0.976878612716763 or  97.6878612716763%\n"
                    ]
                }
            ],
            "source": [
                "# Evaluate the model\n",
                "cm = sklearn.metrics.confusion_matrix(y_test, y_pred); # Test the AI with the test data and make a confusion matrix to compare its guesses with the real values.\n",
                "print(cm); # Print the confusion matrix so we can see how well the AI did and potentially what it had difficulty predicting and needs to improve (not my AI though).\n",
                "print(\"Accuracy:\", sklearn.metrics.accuracy_score(y_test, y_pred), \"or \", str(sklearn.metrics.accuracy_score(y_test, y_pred)*100) + \"%\"); # Print the accuracy of the AI as a decimal and as a percentage. Using the str() function is required to remove the space between the accuracy and the percentage sign."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
